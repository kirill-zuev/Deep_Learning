{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DBhKk93WoI4"
      },
      "source": [
        "# Практическое задание\n",
        "\n",
        "## Данные о студенте\n",
        "\n",
        "1. **ФИО**: Зуев Кирилл Петрович\n",
        "2. **Факультет**: Факультет Космических Исследований\n",
        "3. **Курс**: 005\n",
        "4. **Группа**: 502\n",
        "\n",
        "## Замечания\n",
        "\n",
        "* Заполненный ноутбук необходимо сдать боту\n",
        "* Соблюдаем кодекс чести (по нулям и списавшему, и давшему списать)\n",
        "* Можно (и нужно!) применять для реализации только библиотеку **Numpy**\n",
        "* Ничего, крому Numpy, нельзя использовать для реализации\n",
        "* **Keras** используется только для тестирования Вашей реализации\n",
        "* Если какой-то из классов не проходит приведенные тесты, то соответствующее задание не оценивается\n",
        "* Возможно использование дополнительных (приватных) тестов\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KH_tJiYDWoI6"
      },
      "source": [
        "## Реализация собственного нейросетевого пакета для запуска и обучения нейронных сетей"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-G34Nm2qWoI7"
      },
      "source": [
        "Задание состоит из трёх частей:\n",
        "1. Реализация прямого вывода нейронной сети (5 баллов)\n",
        "2. Реализация градиентов по входу и распространения градиента по сети (5 баллов)\n",
        "3. Реализация градиентов по параметрам и метода обратного распространения ошибки с обновлением парметров сети (10 баллов)\n",
        "\n",
        "Дополнительные баллы можно получить при реализации обучения сети со свёрточными слоями (10 баллов), с транспонированной свёрткой (10 баллов), дополнительного оптимизатора (5 баллов)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dH6Y-bSTWoI8"
      },
      "source": [
        "###  1. Реализация вывода собственной нейронной сети"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "741_ZMEAWoI8"
      },
      "source": [
        "1.1 Внимательно ознакомьтесь с интерфейсом слоя. Любой слой должен содержать как минимум три метода:\n",
        "- конструктор\n",
        "- прямой вывод\n",
        "- обратный вывод, производные по входу и по параметрам"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "ymQL5Xd9WoI8"
      },
      "outputs": [],
      "source": [
        "class Layer(object):\n",
        "    def __init__(self):\n",
        "        self.name = 'Layer'\n",
        "    def forward(self, input_data):\n",
        "        pass\n",
        "    def backward(self, input_data):\n",
        "        return [self.grad_x(input_data), self.grad_param(input_data)]\n",
        "\n",
        "    def grad_x(self, input_data):\n",
        "        pass\n",
        "    def grad_param(self, input_data):\n",
        "        return []\n",
        "\n",
        "    def update_param(self, grads, learning_rate):\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYDFQKv7WoI9"
      },
      "source": [
        "1.2 Ниже предствален интерфейс класса  Network. Обратите внимание на реализацию метода predict, который последовательно обрабатывает входные данные слой за слоем."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "Q6s8b9DmWoI9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "class Network(object):\n",
        "    def __init__(self, layers, loss=None):\n",
        "        self.name = 'Network'\n",
        "        self.layers = layers\n",
        "        self.loss = loss\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        return self.predict(input_data)\n",
        "\n",
        "    def grad_x(self, input_data, labels):\n",
        "        input_mas = []\n",
        "        current_input = input_data\n",
        "        for layer in self.layers:\n",
        "            input_mas.append(layer.grad_x(current_input))\n",
        "            current_input = layer.forward(current_input)\n",
        "        L_y = self.loss.grad_x(current_input, labels)\n",
        "        batch_size = L_y.shape[0]\n",
        "        for layer in reversed(input_mas):\n",
        "            grad = np.array([np.dot(L_y[b], layer[b]) for b in range(batch_size)])\n",
        "            L_y = grad.copy()\n",
        "        return L_y\n",
        "\n",
        "    def grad_param(self, input_data, labels):\n",
        "        input_mas = []\n",
        "        current_input = input_data\n",
        "        for layer in self.layers:\n",
        "            input_mas.append(layer.backward(current_input))\n",
        "            current_input = layer.forward(current_input)\n",
        "        L_y = self.loss.grad_x(current_input, labels)\n",
        "        batch_size = L_y.shape[0]\n",
        "        grad_list =  []\n",
        "        for layer_x, layer_param in reversed(input_mas):\n",
        "            grad0 = []\n",
        "            for grad_param in layer_param:\n",
        "                grad0.append(np.array([np.dot(L_y[b], grad_param[b]) for b in range(batch_size)]))\n",
        "            grad_list.append(grad0)\n",
        "            grad = np.array([np.dot(L_y[b], layer_x[b]) for b in range(batch_size)])\n",
        "            L_y = grad.copy()\n",
        "\n",
        "        return grad_list[::-1]\n",
        "\n",
        "    def update(self, grad_list, learning_rate):\n",
        "        for layer, grad in zip(self.layers, grad_list):\n",
        "            layer.update_param(grad, learning_rate)\n",
        "\n",
        "    def predict(self, input_data):\n",
        "        current_input = input_data\n",
        "        self.input_mas = []\n",
        "        self.input_mas.append(current_input)\n",
        "        for layer in self.layers:\n",
        "            current_input = layer.forward(current_input)\n",
        "            self.input_mas.append(current_input)\n",
        "        return current_input\n",
        "\n",
        "    def calculate_loss(self, input_data, labels):\n",
        "        return self.loss.forward(self.predict(input_data), labels)\n",
        "\n",
        "    def train_step(self, input_data, labels, learning_rate=0.001):\n",
        "        grad_list = self.grad_param(input_data, labels)\n",
        "        self.update(grad_list, learning_rate)\n",
        "\n",
        "\n",
        "    def fit(self, trainX, trainY, validation_split=0.25,\n",
        "            batch_size=1, nb_epoch=1, learning_rate=0.01):\n",
        "\n",
        "        train_x, val_x, train_y, val_y = train_test_split(trainX, trainY,\n",
        "                                                          test_size=validation_split,\n",
        "                                                          random_state=42)\n",
        "        for epoch in range(nb_epoch):\n",
        "            #train one epoch\n",
        "            for i in tqdm(range(int(len(train_x)/batch_size))):\n",
        "                batch_x = train_x[i*batch_size: (i+1)*batch_size]\n",
        "                batch_y = train_y[i*batch_size: (i+1)*batch_size]\n",
        "                self.train_step(batch_x, batch_y, learning_rate)\n",
        "            #validate\n",
        "            val_accuracy = self.evaluate(val_x, val_y)\n",
        "            print('%d epoch: val %.2f' %(epoch+1, val_accuracy))\n",
        "\n",
        "    def evaluate(self, testX, testY):\n",
        "        y_pred = np.argmax(self.predict(testX), axis=1)\n",
        "        y_true = np.argmax(testY, axis=1)\n",
        "        val_accuracy = np.sum((y_pred == y_true))/(len(y_true))\n",
        "        return val_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyTAKqh2WoI-"
      },
      "source": [
        "#### 1.1 Необходимо реализовать метод forward для вычисления следующих слоёв:\n",
        "\n",
        "- DenseLayer\n",
        "- ReLU\n",
        "- Softmax\n",
        "- FlattenLayer\n",
        "- MaxPooling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "gkA2v6RqWoI-"
      },
      "outputs": [],
      "source": [
        "#импорты\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "hIhVo96lWoI_"
      },
      "outputs": [],
      "source": [
        "class DenseLayer(Layer):\n",
        "    def __init__(self, input_dim, output_dim, W_init=None, b_init=None):\n",
        "        self.name = 'Dense'\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        if W_init is None or b_init is None:\n",
        "            self.W = np.random.normal(0, np.sqrt(2/self.input_dim), (self.input_dim, self.output_dim))\n",
        "            self.b = np.zeros(self.output_dim, 'float32')\n",
        "        else:\n",
        "            self.W = W_init\n",
        "            self.b = b_init\n",
        "    def forward(self, input_data):\n",
        "        return np.dot(input_data, self.W) + self.b\n",
        "    def grad_x(self, input_data):\n",
        "        batch_size = input_data.shape[0]\n",
        "        grad = []\n",
        "        for i in range(batch_size):\n",
        "            grad.append(self.W.T)\n",
        "        return np.array(grad)\n",
        "    def grad_b(self, input_data):\n",
        "        bsize = input_data.shape[0]\n",
        "        gradb = np.array([np.eye(self.output_dim, dtype=np.float32) for _ in range(bsize)])\n",
        "        return gradb\n",
        "    def grad_W(self, input_data):\n",
        "        bsize = input_data.shape[0]\n",
        "        gradw = np.zeros((bsize, self.output_dim, self.input_dim*self.output_dim), dtype=np.float32)\n",
        "        for i in range(bsize):\n",
        "          for j in range(self.output_dim):\n",
        "            gradw[i, j, j::self.output_dim] = input_data[i]\n",
        "        return np.array(gradw)\n",
        "\n",
        "    def update_W(self, grad, learning_rate):\n",
        "        self.W -= learning_rate * np.mean(grad, axis=0).reshape(self.W.shape)\n",
        "\n",
        "    def update_b(self, grad,  learning_rate):\n",
        "        self.b -= learning_rate * np.mean(grad, axis=0)\n",
        "\n",
        "    def update_param(self, params_grad, learning_rate):\n",
        "        self.update_W(params_grad[0], learning_rate)\n",
        "        self.update_b(params_grad[1], learning_rate)\n",
        "\n",
        "    def grad_param(self, input_data):\n",
        "        return [self.grad_W(input_data), self.grad_b(input_data)]\n",
        "\n",
        "\n",
        "class ReLU(Layer):\n",
        "    def __init__(self):\n",
        "        self.name = 'ReLU'\n",
        "    def forward(self, input_data):\n",
        "        return np.maximum(0, input_data)\n",
        "    def grad_x(self, input_data):\n",
        "        batch_size, num = input_data.shape\n",
        "        grad = np.zeros((batch_size, num, num))\n",
        "        for i in range(batch_size):\n",
        "            grad[i] = np.diag((input_data[i] > 0).astype(float))\n",
        "        return grad\n",
        "\n",
        "\n",
        "class Softmax(Layer):\n",
        "    def __init__(self):\n",
        "        self.name = 'Softmax'\n",
        "    def forward(self, input_data):\n",
        "        exps = np.exp(input_data - np.max(input_data, axis=-1, keepdims=True))\n",
        "        output = exps / np.sum(exps, axis=-1, keepdims=True)\n",
        "        return output\n",
        "    def grad_x(self, input_data):\n",
        "        softmax_out = self.forward(input_data)\n",
        "        batch_size, num = softmax_out.shape\n",
        "        grad = np.zeros((batch_size, num, num))\n",
        "        for i in range(batch_size):\n",
        "            s = softmax_out[i].reshape(-1, 1)\n",
        "            grad[i] = np.diagflat(s) - np.dot(s, s.T)\n",
        "        return grad\n",
        "\n",
        "\n",
        "class FlattenLayer(Layer):\n",
        "    def __init__(self):\n",
        "        self.name = 'Flatten'\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        return input_data.reshape(input_data.shape[0], -1)\n",
        "    def grad_x(self, input_data):\n",
        "        batch = input_data.shape[0]\n",
        "        size = 1\n",
        "        for i in input_data.shape:\n",
        "            size *= i\n",
        "        size = int(size//batch)\n",
        "        batch_eye = np.array([np.eye(size) for _ in range(batch)])\n",
        "        return batch_eye\n",
        "\n",
        "\n",
        "class MaxPooling(Layer):\n",
        "    def __init__(self, pool_size=2, stride=2):\n",
        "        self.name = 'MaxPooling'\n",
        "        self.pool_size = pool_size\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        batch_size, channels, height, width = input_data.shape\n",
        "        out_height = (height - self.pool_size) // self.stride + 1\n",
        "        out_width = (width - self.pool_size) // self.stride + 1\n",
        "\n",
        "        output = np.zeros((batch_size, channels, out_height, out_width))\n",
        "        self.max_indices = np.zeros_like(input_data, dtype=int)\n",
        "\n",
        "        for b in range(batch_size):\n",
        "            for c in range(channels):\n",
        "                for i in range(out_height):\n",
        "                    for j in range(out_width):\n",
        "                        h_start, h_end = i * self.stride, i * self.stride + self.pool_size\n",
        "                        w_start, w_end = j * self.stride, j * self.stride + self.pool_size\n",
        "                        window = input_data[b, c, h_start:h_end, w_start:w_end]\n",
        "                        output[b, c, i, j] = np.max(window)\n",
        "        return output\n",
        "\n",
        "    def grad_x(self, input_data):\n",
        "        batch_size, channels, height, width = input_data.shape\n",
        "        out_height = (height - self.pool_size) // self.stride + 1\n",
        "        out_width = (width - self.pool_size) // self.stride + 1\n",
        "        max_indices = np.zeros_like(input_data, dtype=int)\n",
        "        jacobian = np.zeros((batch_size, channels*out_height*out_width, channels*height*width))\n",
        "\n",
        "        for b in range(batch_size):\n",
        "            for c in range(channels):\n",
        "                for i in range(out_height):\n",
        "                    for j in range(out_width):\n",
        "                        h_start, h_end = i * self.stride, i * self.stride + self.pool_size\n",
        "                        w_start, w_end = j * self.stride, j * self.stride + self.pool_size\n",
        "                        window = input_data[b, c, h_start:h_end, w_start:w_end]\n",
        "                        max_pos = np.unravel_index(np.argmax(window), window.shape)\n",
        "                        max_indices[b, c, h_start + max_pos[0], w_start + max_pos[1]] = 1\n",
        "                        max_h, max_w = max_pos\n",
        "                        input_idx = c * height * width + (h_start + max_h) * width + (w_start + max_w)\n",
        "                        output_idx = c * out_height * out_width + i * out_width + j\n",
        "                        jacobian[b, output_idx, input_idx] = 1\n",
        "        return jacobian"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Тест для MaxPooling"
      ],
      "metadata": {
        "id": "a9xqs1K5gbVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def numerical_diff_layer(layer, x):\n",
        "    eps = 0.00001\n",
        "    right_answer = []\n",
        "    for i in range(x[0].size):\n",
        "        delta = np.zeros(x[0].size)\n",
        "        delta[i] = eps\n",
        "        delta = delta.reshape(x[0].shape)\n",
        "        diff = (layer.forward(x + delta) - layer.forward(x - delta)) / (2 * eps)\n",
        "        right_answer.append(diff.reshape(x.shape[0], -1).T)\n",
        "    return np.array(right_answer).T\n",
        "\n",
        "def test_layer(layer):\n",
        "    x = np.arange(625).reshape(5, 5, 5, 5)\n",
        "    num_grad = numerical_diff_layer(layer, x)\n",
        "    grad = layer.grad_x(x)\n",
        "    if np.sum(np.abs(num_grad - grad)) < 0.01:\n",
        "        print('Test PASSED')\n",
        "    else:\n",
        "        print('Something went wrong!')\n",
        "        print('Numerical grad is')\n",
        "        print(num_grad)\n",
        "        print('Your gradiend is ')\n",
        "        print(grad)\n",
        "\n",
        "layer = MaxPooling()\n",
        "test_layer(layer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vaRTOoHHVqeD",
        "outputId": "72421bc4-86a9-4868-e385-69d93ad33f6c"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test PASSED\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2stYzWOBWoI_"
      },
      "source": [
        "#### 1.2 Реализуйте теперь свёрточный слой и транспонированную свёртку  (опционально)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "TjktKCrwWoI_"
      },
      "outputs": [],
      "source": [
        "class Conv2DLayer(Layer):\n",
        "    def __init__(self, kernel_size=3, input_channels=2, output_channels=3,\n",
        "                 padding='same', stride=1, K_init=None, b_init=None):\n",
        "        # padding: 'same' или 'valid'\n",
        "        # Работаем с квадратными ядрами, поэтому kernel_size - одно число\n",
        "        # Работаем с единообразным сдвигом, поэтому stride - одно число\n",
        "        # Фильтр размерности [kernel_size, kernel_size, input_channels, output_channels]\n",
        "        self.name = 'Conv2D'\n",
        "        self.kernel_size = kernel_size\n",
        "        self.input_channels = input_channels\n",
        "        self.output_channels = output_channels\n",
        "        if K_init is None or b_init is None:\n",
        "            self.kernel = np.random.normal(0, np.sqrt(2/self.input_channels), (self.kernel_size, self.kernel_size, self.input_channels, self.output_channels))\n",
        "            self.bias = np.zeros(self.output_channels, 'float32')\n",
        "        else:\n",
        "            self.kernel = K_init\n",
        "            self.bias = b_init\n",
        "        self.padding = padding\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        # На входе - четырехмерный тензор вида [batch, input_channels, height, width]\n",
        "        # Вначале нужно проверить на согласование размерностей входных данных и ядра!\n",
        "        # Нужно заполнить Numpy-тензор out\n",
        "        if input_data.shape[2] < self.kernel_size or input_data.shape[3] < self.kernel_size:\n",
        "            raise ValueError(\"h or w < kernel_size\")\n",
        "        if self.padding == 'same':\n",
        "            pad = (self.kernel_size - 1) // 2\n",
        "        else:\n",
        "            pad = 0\n",
        "        if self.padding == 'same':\n",
        "            input_data = np.pad(input_data, ((0,0), (0,0), (pad,pad), (pad,pad)), mode='constant')\n",
        "        batch, c_in, h_in, w_in = input_data.shape\n",
        "        H_out = 1 + (h_in - self.kernel_size) // self.stride\n",
        "        W_out = 1 + (w_in - self.kernel_size) // self.stride\n",
        "        out = np.zeros((batch, self.output_channels, H_out, W_out))\n",
        "        for b in range(batch):\n",
        "            for i in range(H_out):\n",
        "                for j in range(W_out):\n",
        "                    region = input_data[b, :, i * self.stride:i * self.stride + self.kernel_size, j * self.stride:j * self.stride + self.kernel_size]\n",
        "                    out[b, :, i, j] = np.tensordot(region, self.kernel, axes=([1, 2, 0], [0, 1, 2])) + self.bias\n",
        "        return out\n",
        "\n",
        "    def grad_x(self, input_data):\n",
        "        k, s = self.kernel_size, self.stride\n",
        "        if self.padding == 'same':\n",
        "            pad = (self.kernel_size - 1) // 2\n",
        "        else:\n",
        "            pad = 0\n",
        "        if self.padding == 'same':\n",
        "            input_data = np.pad(input_data, ((0,0), (0,0), (pad,pad), (pad,pad)), mode='constant')\n",
        "        batch_size, c_in, h_in, w_in = input_data.shape\n",
        "        h_out = 1 + (h_in - self.kernel_size) // self.stride\n",
        "        w_out = 1 + (w_in - self.kernel_size) // self.stride\n",
        "        c_out = self.output_channels\n",
        "\n",
        "        indh_in = self.get_indin(k, s, h_out)\n",
        "        indh_in = indh_in.reshape(h_out, 1, k, 1)\n",
        "        indw_in = self.get_indin(k, s, w_out)\n",
        "        indw_in = indw_in.reshape(1, w_out, 1, k)\n",
        "        indh_out = np.zeros((h_out, 1, 1, 1), dtype=int)\n",
        "        indw_out = np.zeros((1, w_out, 1, 1), dtype=int)\n",
        "        for i in range(h_out):\n",
        "            indh_out[i, 0, 0, 0] = i\n",
        "        for j in range(w_out):\n",
        "            indw_out[0, j, 0, 0] = j\n",
        "\n",
        "        grad = np.zeros((batch_size, h_out, w_out, h_in, w_in, c_in, c_out))\n",
        "        grad[:, indh_out, indw_out, indh_in, indw_in, :, :] = self.kernel\n",
        "\n",
        "        if self.padding == 'same':\n",
        "            h_in -= 2*pad\n",
        "            w_in -= 2*pad\n",
        "        grad = grad[:, :, :, pad:h_in+pad, pad:w_in+pad, :, :]\n",
        "\n",
        "        grad = grad.transpose(0, 6, 1, 2, 5, 3, 4).reshape(batch_size, c_out * h_out * w_out, c_in * h_in * w_in)\n",
        "        return grad\n",
        "\n",
        "    def grad_kernel(self, input_data):\n",
        "        k, s = self.kernel_size, self.stride\n",
        "        if self.padding == 'same':\n",
        "            pad = (self.kernel_size - 1) // 2\n",
        "        else:\n",
        "            pad = 0\n",
        "        if self.padding == 'same':\n",
        "            input_data = np.pad(input_data, ((0,0), (0,0), (pad,pad), (pad,pad)), mode='constant')\n",
        "        batch_size, c_in, h_in, w_in = input_data.shape\n",
        "        h_out = 1 + (h_in - self.kernel_size) // self.stride\n",
        "        w_out = 1 + (w_in - self.kernel_size) // self.stride\n",
        "        c_out = self.output_channels\n",
        "\n",
        "        indh_in = self.get_indin(k, s, h_out)\n",
        "        h = indh_in.reshape(h_out, 1, k, 1).repeat(w_out, axis=1)\n",
        "        indw_in = self.get_indin(k, s, w_out)\n",
        "        w = indw_in.reshape(1, w_out, 1, k).repeat(h_out, axis=0)\n",
        "        piece = input_data[:, :, h, w].transpose(0, 2, 3, 4, 5, 1)\n",
        "\n",
        "        grad = np.tile(np.eye(c_out), reps=(batch_size, h_out, w_out, k, k, c_in, 1, 1))\n",
        "        grad[:, :, :, :, :, :, -1] = np.expand_dims(piece, axis=-1)\n",
        "\n",
        "        grad = grad.transpose(0, 6, 1, 2, 3, 4, 5, 7)\n",
        "        grad = grad.reshape(batch_size, c_out * h_out * w_out, k * k * c_in * c_out)\n",
        "\n",
        "        return grad\n",
        "\n",
        "    def get_indin(self, k, s, i_out):\n",
        "        ind_in = []\n",
        "        for i in range(i_out):\n",
        "            base_index = s * i\n",
        "            row = []\n",
        "            for j in range(k):\n",
        "                row.append(base_index + j)\n",
        "            ind_in.append(row)\n",
        "        return np.array(ind_in)\n",
        "\n",
        "    def grad_param(self, input_data):\n",
        "        return [self.grad_kernel(input_data)]\n",
        "\n",
        "    def update_param(self, grads, learning_rate):\n",
        "        self.kernel -= learning_rate * np.mean(grads[0], axis=0).reshape(self.kernel.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Тест для Conv2DLayer"
      ],
      "metadata": {
        "id": "UBdYyj-1gjA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layer = Conv2DLayer(4, 5, 5, 'same', 1)\n",
        "test_layer(layer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCa5C4KSiPDq",
        "outputId": "5d3463bb-8d0b-45c8-ac9a-d238be3d7cff"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test PASSED\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "DF0crSvZWoJA"
      },
      "outputs": [],
      "source": [
        "class Conv2DTrLayer(Layer):\n",
        "    def __init__(self, kernel_size=3, input_channels=2, output_channels=3,\n",
        "                 padding=0, stride=1, K_init=None, b_init=None):\n",
        "        # padding: число (сколько отрезать от модифицированной входной карты)\n",
        "        # Работаем с квадратными ядрами, поэтому kernel_size - одно число\n",
        "        # stride - одно число (коэффициент расширения)\n",
        "        # Фильтр размерности [kernel_size, kernel_size, input_channels, output_channels]\n",
        "        self.name = 'Conv2DTr'\n",
        "        self.kernel_size = kernel_size\n",
        "        self.input_channels = input_channels\n",
        "        self.output_channels = output_channels\n",
        "        if K_init is None or b_init is None:\n",
        "            self.kernel = np.random.normal(0, np.sqrt(2/self.input_channels), (self.kernel_size, self.kernel_size, self.input_channels, self.output_channels))\n",
        "            self.bias = np.zeros(self.output_channels, 'float32')\n",
        "        else:\n",
        "            self.kernel = K_init\n",
        "            self.bias = b_init\n",
        "        self.padding = padding\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        # На входе - четырехмерный тензор вида [batch, input_channels, height, width]\n",
        "        # Вначале нужно проверить на согласование размерностей входных данных и ядра!\n",
        "        # Нужно заполнить Numpy-тензор out\n",
        "        c_in = self.input_channels\n",
        "        c_out = self.output_channels\n",
        "        k = self.kernel_size\n",
        "        s = self.stride\n",
        "        p = self.padding\n",
        "        if input_data.shape[2] < k or input_data.shape[3] < k:\n",
        "            raise ValueError(\"h or w < kernel_size\")\n",
        "        batch_size, _ , H_in, W_in = input_data.shape\n",
        "        H_new = H_in + (H_in - 1) * (s - 1) + 2 * (k - p - 1)\n",
        "        W_new = W_in + (W_in - 1) * (s - 1) + 2 * (k - p - 1)\n",
        "\n",
        "        data = np.zeros((batch_size, c_in, H_new, W_new))\n",
        "        h = (k - p - 1) + s * np.arange(H_in).reshape(H_in, 1)\n",
        "        w = (k - p - 1) + s * np.arange(W_in).reshape(1, W_in)\n",
        "        data[:, :, h, w] = input_data\n",
        "        input_data = data\n",
        "        batch_size, _ , H_in, W_in = input_data.shape\n",
        "        H_out, W_out = H_in - k + 1, W_in - k + 1\n",
        "\n",
        "        indh_in = self.get_indin(k, s, H_out)\n",
        "        h = indh_in.reshape(H_out, 1, k, 1).repeat(W_out, axis=1)\n",
        "        indw_in = self.get_indin(k, s, W_out)\n",
        "        w = indw_in.reshape(1, W_out, 1, k).repeat(H_out, axis=0)\n",
        "        piece = input_data[:, :, h, w].transpose(0, 2, 3, 4, 5, 1)\n",
        "        output = (np.tensordot(piece, self.kernel, axes=([3, 4, 5], [0, 1, 2])) + self.bias).transpose(0, 3, 1, 2)\n",
        "        return output\n",
        "\n",
        "    def grad_x(self, input_data):\n",
        "        c_in = self.input_channels\n",
        "        c_out = self.output_channels\n",
        "        k = self.kernel_size\n",
        "        s = self.stride\n",
        "        p = self.padding\n",
        "        if input_data.shape[2] < k or input_data.shape[3] < k:\n",
        "            raise ValueError(\"h or w < kernel_size\")\n",
        "        batch_size, _ , H_in, W_in = input_data.shape\n",
        "        H_new = H_in + (H_in - 1) * (s - 1) + 2 * (k - p - 1)\n",
        "        W_new = W_in + (W_in - 1) * (s - 1) + 2 * (k - p - 1)\n",
        "\n",
        "        data = np.zeros((batch_size, c_in, H_new, W_new))\n",
        "        h = (k - p - 1) + s * np.arange(H_in).reshape(H_in, 1)\n",
        "        w = (k - p - 1) + s * np.arange(W_in).reshape(1, W_in)\n",
        "        data[:, :, h, w] = input_data\n",
        "        input_data = data\n",
        "        batch_size, _ , H_in, W_in = input_data.shape\n",
        "        H_out, W_out = H_in - k + 1, W_in - k + 1\n",
        "\n",
        "        indh_in = self.get_indin(k, s, H_out)\n",
        "        h = indh_in.reshape(H_out, 1, k, 1)\n",
        "        indw_in = self.get_indin(k, s, W_out)\n",
        "        w = indw_in.reshape(1, W_out, 1, k)\n",
        "\n",
        "        indh_out = np.zeros((H_out, 1, 1, 1), dtype=int)\n",
        "        indw_out = np.zeros((1, W_out, 1, 1), dtype=int)\n",
        "        for i in range(H_out):\n",
        "            indh_out[i, 0, 0, 0] = i\n",
        "        for j in range(W_out):\n",
        "            indw_out[0, j, 0, 0] = j\n",
        "\n",
        "        grad = np.zeros((batch_size, H_out, W_out, H_in, W_in, c_in, c_out))\n",
        "        grad[:, indh_out, indw_out, h, w, :, :] = self.kernel\n",
        "\n",
        "        grad = grad.transpose(0, 6, 1, 2, 5, 3, 4)\n",
        "        h_new = (H_in-2*(k - p - 1)-1)//s+1\n",
        "        w_new = (W_in-2*(k - p - 1)-1)//s+1\n",
        "\n",
        "        indh_out = np.zeros((h_new, 1), dtype=int)\n",
        "        indw_out = np.zeros((1, w_new), dtype=int)\n",
        "        for i in range(h_new):\n",
        "            indh_out[i, 0] = i\n",
        "        for j in range(w_new):\n",
        "            indw_out[0, j] = j\n",
        "        indh_out = (k - p - 1) + s * indh_out\n",
        "        indw_out = (k - p - 1) + s * indw_out\n",
        "        grad = grad[:, :, :, :, :, indh_out, indw_out]\n",
        "        grad = grad.reshape(batch_size, c_out * H_out * W_out, -1)\n",
        "        return grad\n",
        "\n",
        "    def grad_kernel(self, input_data):\n",
        "        c_in = self.input_channels\n",
        "        c_out = self.output_channels\n",
        "        k = self.kernel_size\n",
        "        s = self.stride\n",
        "        p = self.padding\n",
        "        if input_data.shape[2] < k or input_data.shape[3] < k:\n",
        "            raise ValueError(\"h or w < kernel_size\")\n",
        "        batch_size, _ , H_in, W_in = input_data.shape\n",
        "        H_new = H_in + (H_in - 1) * (s - 1) + 2 * (k - p - 1)\n",
        "        W_new = W_in + (W_in - 1) * (s - 1) + 2 * (k - p - 1)\n",
        "\n",
        "        data = np.zeros((batch_size, c_in, H_new, W_new))\n",
        "        h = (k - p - 1) + s * np.arange(H_in).reshape(H_in, 1)\n",
        "        w = (k - p - 1) + s * np.arange(W_in).reshape(1, W_in)\n",
        "        data[:, :, h, w] = input_data\n",
        "        input_data = data\n",
        "        batch_size, _ , H_in, W_in = input_data.shape\n",
        "        H_out, W_out = H_in - k + 1, W_in - k + 1\n",
        "\n",
        "        indh_in = self.get_indin(k, s, H_out)\n",
        "        h = indh_in.reshape(H_out, 1, k, 1).repeat(W_out, axis=1)\n",
        "        indw_in = self.get_indin(k, s, W_out)\n",
        "        w = indw_in.reshape(1, W_out, 1, k).repeat(H_out, axis=0)\n",
        "        piece = input_data[:, :, h, w].transpose(0, 2, 3, 4, 5, 1)\n",
        "\n",
        "        grad = np.tile(np.eye(c_out), reps=(batch_size, H_out, W_out, k, k, c_in, 1, 1))\n",
        "        grad[:, :, :, :, :, :, -1] = np.expand_dims(piece, axis=-1)\n",
        "\n",
        "        grad = grad.transpose(0, 6, 1, 2, 3, 4, 5, 7)\n",
        "        grad = grad.reshape(batch_size, c_out * H_out * W_out, k * k * c_in * c_out)\n",
        "\n",
        "        return grad\n",
        "\n",
        "    def get_indin(self, k, s, i_out):\n",
        "        ind_in = []\n",
        "        for i in range(i_out):\n",
        "            base_index = s * i\n",
        "            row = []\n",
        "            for j in range(k):\n",
        "                row.append(base_index + j)\n",
        "            ind_in.append(row)\n",
        "        return np.array(ind_in)\n",
        "\n",
        "    def grad_param(self, input_data):\n",
        "        return [self.grad_kernel(input_data)]\n",
        "\n",
        "    def update_param(self, grads, learning_rate):\n",
        "        self.kernel -= learning_rate * np.mean(grads[0], axis=0).reshape(self.kernel.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Тест для Conv2DTrLayer"
      ],
      "metadata": {
        "id": "AG2rFN37gzMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layer = Conv2DTrLayer(4, 5, 5, 1, 1)\n",
        "test_layer(layer)"
      ],
      "metadata": {
        "id": "Ggpzmnv8Jlm3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41d052f1-57b0-4f97-a439-db5861d39e05"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test PASSED\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBAbitvOWoJA"
      },
      "source": [
        "#### 1.4 Теперь настало время теста.\n",
        "#### Если вы всё сделали правильно, то запустив следующие ячейки у вас должна появиться надпись: Test PASSED\n",
        "\n",
        "Переходить к дальнейшим заданиям не имеем никакого смысла, пока вы не добьётесь прохождение теста\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OC980WhqWoJA"
      },
      "source": [
        "#### Чтение данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5Bf5qrlWoJA",
        "outputId": "52b29039-870e-4afb-fffa-23cd1e9c06d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "(60000, 1, 28, 28) (60000, 10) (10000, 1, 28, 28) (10000, 10)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "np.random.seed(123)  # for reproducibility\n",
        "from keras.utils import to_categorical\n",
        "from keras.datasets import mnist\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], 1, 28, 28)\n",
        "X_test = X_test.reshape(X_test.shape[0], 1, 28, 28)\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "\n",
        "Y_train = to_categorical(y_train, 10)\n",
        "Y_test = to_categorical(y_test, 10)\n",
        "print(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95UPJ6WdWoJB"
      },
      "source": [
        "#### Подготовка моделей"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knA2z1ZSWoJB",
        "outputId": "f26f0047-de8e-4a06-fde5-de744f0a1f95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.8.0\n"
          ]
        }
      ],
      "source": [
        "import keras\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Input\n",
        "from keras.layers import Convolution2D, Conv2D, MaxPooling2D\n",
        "\n",
        "print(keras.__version__)\n",
        "\n",
        "def get_keras_model():\n",
        "    input_image = Input(shape=(1, 28, 28))\n",
        "    pool1 = MaxPooling2D(pool_size=(2,2), data_format='channels_first')(input_image)\n",
        "    flatten = Flatten()(pool1)\n",
        "    dense1 = Dense(10, activation='softmax')(flatten)\n",
        "    model = Model(inputs=input_image, outputs=dense1)\n",
        "\n",
        "    from keras.optimizers import Adam, SGD\n",
        "    sgd = SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer=sgd,\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    history = model.fit(X_train, Y_train, validation_split=0.25,\n",
        "                        batch_size=32, epochs=2, verbose=1)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "Kenwzxx4WoJB"
      },
      "outputs": [],
      "source": [
        "def get_our_model(keras_model):\n",
        "    maxpool = MaxPooling()\n",
        "    flatten = FlattenLayer()\n",
        "    dense = DenseLayer(196, 10, W_init=keras_model.get_weights()[0],\n",
        "                       b_init=keras_model.get_weights()[1])\n",
        "    softmax = Softmax()\n",
        "    net = Network([maxpool, flatten, dense, softmax])\n",
        "    return net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thwyBeicWoJB",
        "outputId": "b6fcf378-539e-480a-a6f7-9c6ae39a8f52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.7659 - loss: 0.8546 - val_accuracy: 0.8910 - val_loss: 0.3825\n",
            "Epoch 2/2\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8894 - loss: 0.3914 - val_accuracy: 0.9011 - val_loss: 0.3436\n"
          ]
        }
      ],
      "source": [
        "keras_model = get_keras_model()\n",
        "our_model = get_our_model(keras_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiMzncEcWoJC",
        "outputId": "72c9da94-b55d-49c9-b6a9-6db5c96d3305"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n"
          ]
        }
      ],
      "source": [
        "keras_prediction = keras_model.predict(X_test)\n",
        "our_model_prediction = our_model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAnxIZWvWoJC",
        "outputId": "35252122-1146-43ff-8955-ed499118b221"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test PASSED\n"
          ]
        }
      ],
      "source": [
        "if np.sum(np.abs(keras_prediction - our_model_prediction)) < 0.01:\n",
        "    print('Test PASSED')\n",
        "else:\n",
        "    print('Something went wrong!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-YKVJ7CWoJC"
      },
      "source": [
        "### 2. Вычисление производных по входу для слоёв нейронной сети"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Rb1-sWUWoJC"
      },
      "source": [
        "В данном задании запрещено использовать численные формулы для вычисления производных."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWoJB7OOWoJD"
      },
      "source": [
        "#### 2.1  Реализуйте метод forward для класса CrossEntropy\n",
        "Напоминание: $$ crossentropy = L(p, y) =  - \\sum\\limits_i y_i log p_i, $$\n",
        "где вектор $(p_1, ..., p_k) $ -  выход классификационного алгоритма, а $(y_1,..., y_k)$ - правильные метки класса в унарной кодировке (one-hot encoding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "cVmMePd-WoJD"
      },
      "outputs": [],
      "source": [
        "class CrossEntropy(object):\n",
        "    def __init__(self, eps=0.00001):\n",
        "        self.name = 'CrossEntropy'\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, input_data, labels):\n",
        "        return -np.sum(labels*np.log(input_data+self.eps), axis=1)\n",
        "\n",
        "    def calculate_loss(self,input_data, labels):\n",
        "        return self.forward(input_data, labels)\n",
        "\n",
        "    def grad_x(self, input_data, labels):\n",
        "        return -labels / (input_data+self.eps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSEU58AhWoJD"
      },
      "source": [
        "#### 2.2  Реализуйте метод grad_x класса CrossEntropy, который возвращает $\\frac{\\partial L}{\\partial p}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atyZlHurWoJD"
      },
      "source": [
        "Проверить работоспособность кода поможет следующий тест:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPNVjXijWoJD",
        "outputId": "177606bc-ce40-46cd-dfaf-40c5baea1c17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test PASSED\n"
          ]
        }
      ],
      "source": [
        "def numerical_diff_net(net, x, labels):\n",
        "    eps = 0.00001\n",
        "    right_answer = []\n",
        "    for i in range(len(x[0])):\n",
        "        delta = np.zeros(len(x[0]))\n",
        "        delta[i] = eps\n",
        "        diff = (net.calculate_loss(x + delta, labels) - net.calculate_loss(x-delta, labels)) / (2*eps)\n",
        "        right_answer.append(diff)\n",
        "    return np.array(right_answer).T\n",
        "\n",
        "def test_net(net):\n",
        "    x = np.array([[1, 2, 3], [2, 3, 4]])\n",
        "    labels = np.array([[0.3, 0.2, 0.5], [0.3, 0.2, 0.5]])\n",
        "    num_grad = numerical_diff_net(net, x, labels)\n",
        "    grad = net.grad_x(x, labels)\n",
        "    if np.sum(np.abs(num_grad - grad)) < 0.01:\n",
        "        print('Test PASSED')\n",
        "    else:\n",
        "        print('Something went wrong!')\n",
        "        print('Numerical grad is')\n",
        "        print(num_grad)\n",
        "        print('Your gradiend is ')\n",
        "        print(grad)\n",
        "\n",
        "loss = CrossEntropy()\n",
        "test_net(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7yPTn6oWoJD"
      },
      "source": [
        "#### 2.3  Реализуйте метод grad_x класса Softmax, который возвращает $\\frac{\\partial Softmax}{\\partial x}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwVGi6JmWoJD"
      },
      "source": [
        "Проверить работоспособность кода поможет следующий тест:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84AwmRbIWoJE",
        "outputId": "d9a292a2-6f84-4faf-de27-d160f75f732f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test PASSED\n"
          ]
        }
      ],
      "source": [
        "def numerical_diff_layer(layer, x):\n",
        "    eps = 0.00001\n",
        "    right_answer = []\n",
        "    for i in range(len(x[0])):\n",
        "        delta = np.zeros(len(x[0]))\n",
        "        delta[i] = eps\n",
        "        diff = (layer.forward(x + delta) - layer.forward(x-delta)) / (2*eps)\n",
        "        right_answer.append(diff.T)\n",
        "    return np.array(right_answer).T\n",
        "\n",
        "def test_layer(layer):\n",
        "    x = np.array([[1, 2, 3], [2, -3, 4]])\n",
        "    num_grad = numerical_diff_layer(layer, x)\n",
        "    grad = layer.grad_x(x)\n",
        "    if np.sum(np.abs(num_grad - grad)) < 0.01:\n",
        "        print('Test PASSED')\n",
        "    else:\n",
        "        print('Something went wrong!')\n",
        "        print('Numerical grad is')\n",
        "        print(num_grad)\n",
        "        print('Your gradiend is ')\n",
        "        print(grad)\n",
        "\n",
        "layer = Softmax()\n",
        "test_layer(layer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxqy_d_2WoJE"
      },
      "source": [
        "#### 2.4  Реализуйте метод grad_x для классов ReLU и DenseLayer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJbrc4tEWoJE",
        "outputId": "9c6cd973-df4b-4504-bef3-87193d116cdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test PASSED\n"
          ]
        }
      ],
      "source": [
        "layer = ReLU()\n",
        "test_layer(layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "b687lx0iWoJE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fe4e06a-91d4-4441-bb5a-b365c83bf66d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test PASSED\n"
          ]
        }
      ],
      "source": [
        "layer = DenseLayer(3,4)\n",
        "test_layer(layer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ri7rJS6WoJF"
      },
      "source": [
        "#### 2.5 (4 балла) Для класса Network реализуйте метод grad_x, который должен реализовывать взятие производной от лосса по входу"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "7HwPc7VxWoJF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db980342-bc88-46eb-8e7b-8ff92a76abaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test PASSED\n"
          ]
        }
      ],
      "source": [
        "net = Network([DenseLayer(3, 10), ReLU(), DenseLayer(10, 3), Softmax()], loss=CrossEntropy())\n",
        "test_net(net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0umK1uVAWoJF"
      },
      "source": [
        "### 3. Реализация градиентов по параметрам и метода обратного распространения ошибки с обновлением парметров сети"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JXO5v0KWoJa"
      },
      "source": [
        "#### 3.1  Реализуйте функции grad_b и grad_W. При подготовке теста grad_W предполагается, что W является отномерным вектором."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "lLMw0cu9WoJa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e47f05b-f260-453f-c53f-ee8f57fa1e88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test PASSED\n"
          ]
        }
      ],
      "source": [
        "def numerical_grad_b(input_size, output_size, b, W, x):\n",
        "    eps = 0.00001\n",
        "    right_answer = []\n",
        "    for i in range(len(b)):\n",
        "        delta = np.zeros(b.shape)\n",
        "        delta[i] = eps\n",
        "        dense1 = DenseLayer(input_size, output_size, W_init=W, b_init=b+delta)\n",
        "        dense2 = DenseLayer(input_size, output_size, W_init=W, b_init=b-delta)\n",
        "        diff = (dense1.forward(x) - dense2.forward(x)) / (2*eps)\n",
        "        right_answer.append(diff.T)\n",
        "    return np.array(right_answer).T\n",
        "\n",
        "def test_grad_b():\n",
        "    input_size = 3\n",
        "    output_size = 4\n",
        "    W_init = np.random.random((input_size, output_size))\n",
        "    b_init = np.random.random((output_size,))\n",
        "    x = np.random.random((2, input_size))\n",
        "\n",
        "    dense = DenseLayer(input_size, output_size, W_init, b_init)\n",
        "    grad = dense.grad_b(x)\n",
        "\n",
        "    num_grad = numerical_grad_b(input_size, output_size, b_init, W_init, x)\n",
        "    if np.sum(np.abs(num_grad - grad)) < 0.01:\n",
        "        print('Test PASSED')\n",
        "    else:\n",
        "        print('Something went wrong!')\n",
        "        print('Numerical grad is')\n",
        "        print(num_grad)\n",
        "        print('Your gradiend is ')\n",
        "        print(grad)\n",
        "\n",
        "test_grad_b()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "3Gjl3YYRWoJa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "033cc6ef-6e93-4420-9fd3-b9700a0d64c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test PASSED\n"
          ]
        }
      ],
      "source": [
        "def numerical_grad_W(input_size, output_size, b, W, x):\n",
        "    eps = 0.00001\n",
        "    right_answer = []\n",
        "    for i in range(W.shape[0]):\n",
        "        for j in range(W.shape[1]):\n",
        "            delta = np.zeros(W.shape)\n",
        "            delta[i, j] = eps\n",
        "            dense1 = DenseLayer(input_size, output_size, W_init=W+delta, b_init=b)\n",
        "            dense2 = DenseLayer(input_size, output_size, W_init=W-delta, b_init=b)\n",
        "            diff = (dense1.forward(x) - dense2.forward(x)) / (2*eps)\n",
        "            right_answer.append(diff.T)\n",
        "    return np.array(right_answer).T\n",
        "\n",
        "def test_grad_W():\n",
        "    input_size = 3\n",
        "    output_size = 4\n",
        "    W_init = np.random.random((input_size, output_size))\n",
        "    b_init = np.random.random((4,))\n",
        "    x = np.random.random((2, input_size))\n",
        "\n",
        "    dense = DenseLayer(input_size, output_size, W_init, b_init)\n",
        "    grad = dense.grad_W(x)\n",
        "\n",
        "    num_grad = numerical_grad_W(input_size, output_size, b_init, W_init, x)\n",
        "    if np.sum(np.abs(num_grad - grad)) < 0.01:\n",
        "        print('Test PASSED')\n",
        "    else:\n",
        "        print('Something went wrong!')\n",
        "        print('Numerical grad is')\n",
        "        print(num_grad)\n",
        "        print('Your gradiend is ')\n",
        "        print(grad)\n",
        "\n",
        "test_grad_W()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-x5buodxWoJb"
      },
      "source": [
        "#### 3.2 Полностью реализуйте метод обратного распространения ошибки в функции train_step класса Network\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaXnkAKGWoJb"
      },
      "source": [
        "Рекомендуем реализовать сначала функцию Network.grad_param(), которая возвращает список длиной в количество слоёв и элементом которого является список градиентов по параметрам.\n",
        "После чего, имея список градиентов, написать функцию обновления параметров для каждого слоя.\n",
        "\n",
        "Совет: рекомендуем написать тест для кода подсчета градиента по параметрам, чтобы быть уверенным в том, что градиент через всю сеть считается правильно\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMZrZ-IMWoJb"
      },
      "source": [
        "#### 3.3 Ознакомьтесь с реализацией функции fit класса Network. Запустите обучение модели. Если всё работает правильно, то точность на валидации должна будет возрастать"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "9JWIIYSyWoJb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a5f5f27-45b6-4221-9bd6-bdea454815c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 937/937 [00:05<00:00, 177.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 epoch: val 0.85\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 937/937 [00:04<00:00, 231.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2 epoch: val 0.87\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 937/937 [00:04<00:00, 228.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3 epoch: val 0.88\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 937/937 [00:05<00:00, 187.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4 epoch: val 0.89\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 937/937 [00:04<00:00, 227.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5 epoch: val 0.89\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "net = Network([DenseLayer(784, 10), Softmax()], loss=CrossEntropy())\n",
        "trainX = X_train.reshape(len(X_train), -1)\n",
        "net.fit(trainX[::3], Y_train[::3], validation_split=0.25,\n",
        "            batch_size=16, nb_epoch=5, learning_rate=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "6rjpyZCvWoJb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "902a394e-9138-42b6-8b7a-88126b00a73d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 468/468 [00:09<00:00, 49.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 epoch: val 0.28\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 468/468 [00:09<00:00, 50.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2 epoch: val 0.43\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 468/468 [00:09<00:00, 50.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3 epoch: val 0.53\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 468/468 [00:09<00:00, 49.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4 epoch: val 0.62\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 468/468 [00:09<00:00, 49.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5 epoch: val 0.68\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "net = Network([DenseLayer(784, 20), ReLU(), DenseLayer(20, 10), Softmax()], loss=CrossEntropy())\n",
        "trainX = X_train.reshape(len(X_train), -1)\n",
        "net.fit(trainX[::6], Y_train[::6], validation_split=0.25,\n",
        "            batch_size=16, nb_epoch=5, learning_rate=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abhhhM85WoJc"
      },
      "source": [
        "#### 3.5 Продемонстрируйте, что ваша реализация позволяет обучать более глубокие нейронные сети"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Нейронная сеть состоит из слоев\n",
        "\n",
        "1. Conv2DLayer\n",
        "2. Conv2DLayer\n",
        "3. Conv2DLayer\n",
        "4. MaxPooling\n",
        "5. Conv2DTrLayer\n",
        "6. FlattenLayer\n",
        "7. DenseLayer\n",
        "8. ReLU\n",
        "9. DenseLayer\n",
        "10. Softmax"
      ],
      "metadata": {
        "id": "t_FrNPRwGsLD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "gbB7B67wWoJc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f5e3c1e-a995-4da0-fcfb-bcc82eebc89c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 468/468 [08:24<00:00,  1.08s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 epoch: val 0.28\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 468/468 [08:27<00:00,  1.08s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2 epoch: val 0.37\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 468/468 [08:27<00:00,  1.08s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3 epoch: val 0.43\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 468/468 [08:23<00:00,  1.08s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4 epoch: val 0.43\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 468/468 [08:18<00:00,  1.07s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5 epoch: val 0.44\n"
          ]
        }
      ],
      "source": [
        "net = Network([\n",
        "    Conv2DLayer(kernel_size=3, input_channels=1, output_channels=1, padding='same', stride=1),\n",
        "    Conv2DLayer(kernel_size=3, input_channels=1, output_channels=1, padding='valid', stride=1),\n",
        "    Conv2DLayer(kernel_size=3, input_channels=1, output_channels=1, padding='same', stride=1),\n",
        "    MaxPooling(),\n",
        "    Conv2DTrLayer(3, 1, 1, 0, 1),\n",
        "    FlattenLayer(),\n",
        "    DenseLayer(15 * 15, 100),\n",
        "    ReLU(),\n",
        "    DenseLayer(100, 10),\n",
        "    Softmax()\n",
        "], loss=CrossEntropy())\n",
        "\n",
        "net.fit(X_train[::6], Y_train[::6], validation_split=0.25,\n",
        "            batch_size=16, nb_epoch=5, learning_rate=0.001)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}